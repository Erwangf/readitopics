package labeling;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.TreeSet;

import cc.mallet.types.InstanceList;
import core.CleanWord;
import core.ForIndexing;
import core.TFSort;
import core.TermValuePair;
import topicmodeling.LDATopicModel;

public class COrderLabeler extends TopicLabelerSkeleton
{

	public static final int ZERO_NORM = 1;
	public static final int TERM_NORM = 2;
	private static InstanceList instances;

	private static LDATopicModel topic_model;
	public LDATopicModel top;
	private int norm;

	public COrderLabeler(LDATopicModel topic_word, String dir)
	//ArrayList<TreeSet<IDSorter>> topicSortedWords, ArrayList<TFSort> topicSortedDocs)
	{
		super(topic_word, dir); //, topicSortedWords, topicSortedDocs);
		top=topic_word;
		this.norm = ZERO_NORM;
	}

	public void setNorm(int i)
	{
		norm = i;
	}

	private void computeLabel(int i) throws IOException
	{
		System.out.print("topic " + i +"-");
		// get the label candidates for topic i
		ArrayList<ForIndexing> c_for_topic;
		if (different_candidates)
			c_for_topic = candidates[i];
		else
			c_for_topic = filter_candidates(i, num_words_for_filtering);
		TFSort res = new TFSort();

		//a modifer
		ArrayList<GetBiot> mots = GetBiot.Lid_read("LIDF_value");
//		System.out.println(mots.size());
		ArrayList<String> mb = new ArrayList<String>();
		for (GetBiot c : mots){
			mb.add(c.mot);
		}



		for (ForIndexing c : c_for_topic)
		{
			String[] split = c.getTerm().split(" ");

			//On va d'abord calculer la somme des proba de génération du candidat pour chaque topic
			//on enlevera la proba de génération pour le topic en cours ensuite pour eviter les tests à chaque boucle	


			double sum = 0;
			for (String w : split){
				double proba_u = topic_word.getProbaWordGivenTopic(w, i);
				if (proba_u == -1){
					proba_u=0;
				}
				double gene_totale = 0;
				for (int j = 0;j<k;j++)
				{
					double proba_tu = topic_word.getProbaWordGivenTopic(w, j);
					if (proba_tu == -1){
						proba_tu=0;
					}
					gene_totale+=proba_tu;
				}
				gene_totale=(gene_totale - proba_u)/(topic_word.numTopics()-1);
				if ((proba_u != 0) && (gene_totale != 0))
				{
					sum +=Math.log(proba_u/gene_totale)/split.length;
					//System.out.print(num+"/"+den+";;");
				}
				else{
					sum = 0;
				}
			}
			if (norm==TERM_NORM){
				try{
					for (GetBiot gt : mots ){
						if (gt.mot.indexOf(c.getTerm())!=-1){	
							if (mots.get(mb.indexOf(c.getTerm())).score<gt.score){
								sum=0;
							}
						}
					}
//					sum *= (0.1) * Math.log(mots.get(mb.indexOf(c.getTerm())).score);
				}catch(ArrayIndexOutOfBoundsException e){
//					System.out.println(c.getTerm());
					sum = 0;
				}
			}
			//				if (num == -1){
			//					num=0;
			//				}
			//				if (num2 == -1){
			//					num2=0.00000000000001;
			//				}
			//				sum*=num;
			//				sum2 += Math.log(num/num2);
			//			double p_total_l=0;
			//			for (int j = 0;j<k;j++)
			//			{
			//				double proba_topic = 1;
			//				for (String w : split)
			//				{
			//					double num=0;
			////					if(topic_word.getCurrentTopicModel().getAlphabet().contains(w)){
			////						num = topic_word.getProbaWordGivenTopic(w, j);
			////					}
			//					num = topic_word.getProbaWordGivenTopic(w, j);
			//					if (num == -1)
			//						num=0;
			//					else
			//						proba_topic*=num;
			//				}
			//				p_total_l += Math.pow(proba_topic, 1/split.length);
			//			}
			//			//Computing for the running topic
			//			double sum = 1;
			//			double sum2 = 0;
			//			//computing the odd of split being generated by topic i
			//			for (String w : split)
			//			{
			//				double num=0;
			//				double num2=0;
			////				if(topic_word.getCurrentTopicModel().getAlphabet().contains(w)){
			////					num = topic_word.getProbaWordGivenTopic(w, i);
			////				}
			//				num = topic_word.getProbaWordGivenTopic(w, i);
			//				num2 = topic_word.getProbaWord(w);
			//				if (num == -1){
			//					num=0;
			//				}
			//				if (num2 == -1){
			//					num2=0.00000000000001;
			//				}
			//				sum*=num;
			//				sum2 += Math.log(num/num2);
			//			}
			//			sum = Math.pow(sum, 1/split.length);
			//			double moy_proba_gene;
			//			if (sum != 0)
			//			{
			//				//denominateur(moyenne): 
			//				//On enleve la proba que le terme soit généré par le topic à la prob générale de pondération
			//				p_total_l-=sum;
			//				//probleme pour ponderer le cas ou le terme n'est pas généré par les autres topics...	
			//				if(p_total_l==0){
			//					moy_proba_gene=0.00000000001;
			//				}
			//				else{
			//					moy_proba_gene = p_total_l/(top.numTopics()-1);
			//				}		
			//numerateur
			//score
			//La on calcule le ratio qu'on normalise en prenant la racine cième, c etant la taille du mot
			//de cette façon on ne donne pas d'avantage aux mots courts				
			//			try{
			//				if (norm == ZERO_NORM){
			//					sum /=moy_proba_gene;	
			//					//								+(Math.log(Math.log(mots.get(mb.indexOf(c)).score)));
			//				}
			//				else{
			//					sum=sum2;
			//					//						sum = Math.pow(sum,1/split.length) * 0.005* (Math.log(mots.get(mb.indexOf(c)).score));
			//				}
			//			}catch(ArrayIndexOutOfBoundsException e){
			//				sum = 0;
			//			}
			//		}
			res.add(c.getTerm(), sum);	
		}


		int done = 0;
		TreeSet<TermValuePair> sorted_list = res.getList();
		Iterator<TermValuePair> iter = sorted_list.iterator(); 
		while ((iter.hasNext()) && (done < max_top_terms_calculated))
		{
			TermValuePair tvp = iter.next();
			TermValuePair tvpout = new TermValuePair(CleanWord.TCleaner(tvp.getTerm()),tvp.getValue());
			topicLabels[i].add(tvpout);
			done++;
		}
	}

	@Override
	public void computeLabels() {
		for (int i=0; i<k; i++)
		{
			try {
				computeLabel(i);

			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		}
	}

	public String getName()
	{	
		String s = "t";
		if (norm == ZERO_NORM)
			s = "z";
		return "c-order_" + s;
	}

	public String getShortName()
	{
		//return "0" + norm;
		String s = "t";
		if (norm == ZERO_NORM)
			s = "z";
		return "go"+s;
	}

//	//	//////////////////MAIn for testing
//	public static void main(String[] args) throws NoModel, IOException {
//		loadData("C:/Users/antoi/git/topiclabeling/LDA/euf3m.us");
//
//		String model_file = "2017_05_02";
//
//		System.out.println("Initiate topic labeling");
//		System.out.println("Load topic model: " + model_file);
//
//		// load the first topic model in the directory
//		LDATopicModel.loadTopicModels(model_file);
//		topic_model = LDATopicModel.getFirstTopicModel();
//
//
//		long startTime;
//		long estimatedTime;
//		startTime = System.nanoTime();
//
//		estimatedTime = System.nanoTime() - startTime;		
//		System.out.println(" (" + TimeUnit.NANOSECONDS.toMillis(estimatedTime) + " ms)");
//		startTime = System.nanoTime();		
//		COrderLabeler labeler9 = new COrderLabeler(topic_model, model_file);
//		labeler9.setNorm(COrderLabeler.TERM_NORM);
//		labeler9.setNBtopterms(5);
//		System.out.print("C-order labeling ");
//		try {
//			labeler9.import_labels();
//		} catch (ClassNotFoundException | IOException e1) {
//			System.out.print("\nLabeling not found: compute it from scratch");
//			labeler9.setNBwords_for_filtering(10);
//			labeler9.addCandidates(MonVocabulaire.getActivatedTerms());
//			labeler9.computeLabels();
//			labeler9.export_labels();
//		}
//
//		File myfile = new File("D:\\Bureau\\labelsCO.json");
//		String sb = "";
//		BufferedWriter writer = new BufferedWriter(new FileWriter(myfile));
//		for (int i=0;i<topic_model.numTopics();i++){
//			sb+=("\t\t{\n");
//			sb+=("\t\t\"topicid\" : " + i +",\n");
//			sb+=(",\n");
//			sb+=("\t\t\"");
//			sb+=(labeler9.getName() + "\" : [\n");
//			sb+=("\t\t\t");
//			sb+=(labeler9.getLabelJSON(i, true));
//			sb+=("\n\t\t]");
//
//			sb+=("\n\t\t}");
//			sb+=("\n");
//		}
//		writer.write(sb);
//		writer.flush();
//		writer.close();
//		estimatedTime = System.nanoTime() - startTime;		
//		System.out.println(" (" + TimeUnit.NANOSECONDS.toMillis(estimatedTime) + " ms)");
//
//
//	}
//
//	private static void loadData(String config) throws IOException
//	{
//
//		// check configuration file
//
//		InputStream inputStream;
//		inputStream = new FileInputStream(config);
//
//		if (inputStream != null)
//			LoadConfigFile.loadConfig(inputStream);
//
//		if (LoadDataset.isRawData())
//			LoadDataset.extractFullCorpus(LoadDataset.getPath() + Constantes.separateur + Constantes.DATA_DIR + Constantes.separateur + LoadDataset.getRawData());
//
//		//System.out.println("Weighting scheme: " + LoadConfigFile.getVocTypes()[0]);
//		System.out.print("Load dataset: " + LoadDataset.getDataName());
//
//		long startTime = System.nanoTime();
//		LoadDataset.extractDocs();		
//		long estimatedTime = System.nanoTime() - startTime;		
//		System.out.println(" (" + TimeUnit.NANOSECONDS.toMillis(estimatedTime) + " ms)");
//
//		String biotex_input_dir = LoadDataset.getPath() + Constantes.separateur + Constantes.RESULT_DIR + Constantes.separateur + LoadDataset.getDataName() + Constantes.separateur + "biotex";
//		String path = biotex_input_dir + Constantes.separateur + LoadConfigFile.getVocTypes()[0] + Constantes.separateur;
//		MonVocabulaire.indexing(path);
//
//		/* set the ID used by LDA to the documents */
//		MyDocument.setInternalIDforLDA();
//
//		/* compute the temporal distribution of documents */
//		MyDocument.compute_distrib_temp_doc();
//
//		/* compute p(d) for each period and for the whole period */
//		MyDocument.compute_proba_docs();		
//
//		System.out.println("Compute word distribution over time");
//		MonVocabulaire.computeDistribTemporelle();
//
//		// set which vocabulary is used for LDA
//		//int s = LoadConfigFile.getVocSizes()[0];
//		//MonVocabulaire.setVocabAllWords(biotex_input_dir, LoadConfigFile.getVocTypes()[0], LoadConfigFile.getMinDocs(), LoadConfigFile.getMinTF());
//		//MonVocabulaire.setVocabAllWords(LoadConfigFile.getMinDocs(), LoadConfigFile.getMinTF());
//		MonVocabulaire.setVocabAllTerms(biotex_input_dir, LoadConfigFile.getVocTypes()[0], LoadConfigFile.getMinDocs(), LoadConfigFile.getMinTF());
//
//		instances = RunLDA.preprocessForTopicModeling();
//
//	}
//
	private void Biot() {
		// TODO Auto-generated method stub
	
	}
}
